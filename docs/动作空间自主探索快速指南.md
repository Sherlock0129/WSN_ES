# 动作空间自主探索快速指南

## 🎯 目标

让深度学习调度器**自己寻找**最优的传输时长，而不是人工限定。

## 🔄 三种方案对比

### 方案1：DurationAwareLyapunovScheduler（传统优化）

```python
scheduler_type: str = "DurationAwareLyapunovScheduler"
enable_dqn: bool = False
enable_ddpg: bool = False
```

**特点**：
- ✓ 无需训练，即用即得
- ✓ 可解释性强（基于Lyapunov理论）
- ✗ 每次遍历1-5分钟，计算量大
- ✗ 只能选择整数时长

**适用**：快速验证、理论研究

### 方案2：DQN（离散动作，固定选项）

```python
enable_dqn: bool = True
dqn_training_mode: bool = True
dqn_action_dim: int = 10  # 10个离散选项：1-10分钟
```

**特点**：
- ✓ 训练快（50回合）
- ✓ 稳定性好
- ✗ 只能选10个固定值
- ✗ 动作粒度粗（1分钟间隔）

**适用**：快速原型、资源有限

### 方案3：DDPG（连续动作，自主探索）⭐ 推荐

```python
enable_ddpg: bool = True
ddpg_training_mode: bool = True
ddpg_action_min: float = 1.0   # 自定义下界
ddpg_action_max: float = 10.0  # 自定义上界
```

**特点**：
- ✓ 连续动作（如5.23分钟）
- ✓ 完全自主探索最优值
- ✓ 理论上性能最优
- ✗ 训练时间长（100回合）
- ✗ 训练难度大

**适用**：追求最优性能、研究创新

## ⚡ 快速开始

### 使用DDPG自主探索（推荐）

**1. 修改配置**（`src/config/simulation_config.py`）：

```python
# 启用DDPG
enable_dqn: bool = False
enable_ddpg: bool = True
ddpg_training_mode: bool = True

# 定义探索范围（让DDPG在这个范围内自己寻找）
ddpg_action_min: float = 1.0   # 最少1分钟
ddpg_action_max: float = 10.0  # 最多10分钟

# 训练参数
ddpg_training_episodes: int = 100
```

**2. 运行训练**：

```bash
python src/sim/refactored_main.py
```

**3. 观察学习过程**：

```
训练回合 1/100
  - 选择时长: 3.45, 8.92, 2.17, ...（随机探索）

训练回合 50/100
  - 选择时长: 5.23, 5.67, 4.89, ...（开始收敛）

训练回合 100/100
  - 选择时长: 5.2, 5.3, 5.1, ...（稳定在最优值）
```

**4. 分析结果**：

查看 `duration_statistics.html`：
- 看到时长主要集中在5-6分钟 → 学到了最优值
- 看到分布仍然很广 → 需要更多训练

## 📊 配置模板

### 模板1：保守探索（推荐初学者）

```python
# 小范围，快速收敛
enable_ddpg: bool = True
ddpg_action_min: float = 1.0
ddpg_action_max: float = 5.0
ddpg_training_episodes: int = 50
```

### 模板2：标准探索（推荐通用）

```python
# 中等范围，平衡性能
enable_ddpg: bool = True
ddpg_action_min: float = 1.0
ddpg_action_max: float = 10.0
ddpg_training_episodes: int = 100
```

### 模板3：激进探索（研究场景）

```python
# 大范围，寻找创新策略
enable_ddpg: bool = True
ddpg_action_min: float = 0.5
ddpg_action_max: float = 20.0
ddpg_training_episodes: int = 200
```

## 🔧 调试检查清单

### ✅ 配置检查

```bash
python check_scheduler_config.py
```

应该看到：
```
【预期使用的调度器】
  ✓ DDPG调度器（深度强化学习）
    - 训练模式: True
    - 动作范围: [1.0, 10.0] 分钟
```

### ✅ GPU检查

```bash
python check_gpu.py
```

应该看到：
```
✓ GPU可用，DQN训练将使用GPU加速
GPU设备名称: NVIDIA GeForce RTX 3070 Ti Laptop GPU
```

### ✅ 训练日志检查

运行时应该看到：
```
[DDPG] 使用设备: cuda:0
[DDPG] 动作空间范围: [1.0, 10.0] 分钟

训练统计:
  - 平均Actor损失: < 100（正常）
  - 平均Critic损失: < 500（正常）
```

## 📈 性能预期

### 训练损失

| 回合 | Actor损失 | Critic损失 | 说明 |
|------|----------|-----------|------|
| 1-20 | 50-200 | 200-800 | 探索阶段 |
| 20-60 | 20-100 | 100-400 | 学习阶段 |
| 60-100 | 5-50 | 50-200 | 收敛阶段 |

### 动作收敛

| 回合 | 时长分布 | 标准差 |
|------|---------|-------|
| 1-20 | 均匀分布 | ~3.0 |
| 20-60 | 开始集中 | ~2.0 |
| 60-100 | 高度集中 | ~0.5 |

## 🎓 理论优势

### 为什么连续动作更好？

**示例场景**：

```
最优传输时长 = 5.5分钟

DQN的选择：
  - 可选：5分钟（1500J）或 6分钟（1800J）
  - 误差：±10%能量

DDPG的选择：
  - 可选：5.5分钟（1650J）
  - 误差：理论上可达±0.01%
```

### 自适应学习

DDPG可以学到**依赖状态的策略**：

```
状态1（能量差异大）：
  → DDPG输出：8.5分钟（长传输）

状态2（能量均衡）：
  → DDPG输出：2.3分钟（短传输）

状态3（低能量节点多）：
  → DDPG输出：6.7分钟（中等传输）
```

**DQN/DurationAware做不到这么精细！**

## ⚠️ 注意事项

### 1. 训练时间

DDPG训练比DQN慢约**2-3倍**：
- DQN: 50回合 ≈ 30分钟
- DDPG: 100回合 ≈ 2小时

### 2. 超参数敏感性

DDPG对超参数更敏感：
- 学习率不当 → 发散
- 噪声过大 → 不收敛
- 奖励尺度错误 → 训练失败

### 3. 需要足够数据

建议：
- 训练回合 ≥ 100
- 缓冲区容量 ≥ 10000
- 每回合步数 ≥ 1000

## 📝 完整配置示例

```python
# ========== DDPG自主探索配置 ==========

# 调度器选择
enable_dqn: bool = False
enable_ddpg: bool = True

# DDPG训练配置
ddpg_training_mode: bool = True
ddpg_training_episodes: int = 100
ddpg_save_interval: int = 10
ddpg_model_path: str = "ddpg_model.pth"

# 动作空间配置（核心：让DDPG自己探索）
ddpg_action_dim: int = 1
ddpg_action_min: float = 1.0   # 下界
ddpg_action_max: float = 10.0  # 上界

# 网络超参数
ddpg_actor_lr: float = 1e-4
ddpg_critic_lr: float = 1e-3
ddpg_gamma: float = 0.99
ddpg_tau: float = 0.001
ddpg_buffer_capacity: int = 10000

# 仿真配置
time_steps: int = 10080  # 7天
enable_k_adaptation: bool = False
fixed_k: int = 1

# 被动传能
passive_mode: bool = True
```

## ✅ 验证成功的标志

### 训练成功

1. ✅ 损失下降：Critic损失从~500降到~50
2. ✅ 动作收敛：时长集中在某个范围（标准差<1.0）
3. ✅ 性能提升：能量方差降低、传输效率提高

### 学到有效策略

1. ✅ 测试模式表现好：不比训练模式差
2. ✅ 动作合理：不是极端值（不全是1或10）
3. ✅ 可重复：多次运行结果一致

## 🚀 总结

**DDPG让深度学习网络在[1, 10]分钟范围内自由探索，完全自主地学习最优传输时长！**

核心优势：
- 🎯 连续值输出（5.23分钟而非5分钟）
- 🔍 自主探索（不需要遍历）
- 📈 自适应策略（根据状态动态调整）
- 💪 理论最优（在足够训练后）

**现在就开始训练，让DDPG自己发现最优策略！** 🎉


